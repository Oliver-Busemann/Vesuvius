{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1928a4d3",
   "metadata": {},
   "source": [
    "# Notebook to create the Cross-Validation-Setup & train a first model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a60e63e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from glob import glob\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchinfo import summary\n",
    "from copy import deepcopy\n",
    "import segmentation_models_pytorch as smp\n",
    "from torchinfo import summary\n",
    "import random\n",
    "from copy import deepcopy\n",
    "import albumentations as A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5d7a286",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = '/home/olli/Projects/Kaggle/Vesuvius'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ec0be97",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_data = os.path.join(folder, 'Data', 'Preprocessed', 'Cropped_Regions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5dff864",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob(folder_data + '/*.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65b5b158",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "137"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d6fc227",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort them in the correct order to later get the same results\n",
    "files.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328e0ff8",
   "metadata": {},
   "source": [
    "# 1) Create 5 folds from the 137 cropped parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a738d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folds(files):\n",
    "    \n",
    "    files_copy = deepcopy(files)\n",
    "    \n",
    "    # shuffle them with the same random seed to get identical results\n",
    "    random.Random(42).shuffle(files_copy)\n",
    "    \n",
    "    # each fold will consist of 27, 27, 27, 28, 28 files\n",
    "    num_per_fold = [27, 27, 27, 28, 28]\n",
    "    folds = [f'fold_{i}' for i in range(5)]\n",
    "    \n",
    "    for num, fold in zip(num_per_fold, folds):\n",
    "        globals()[fold] = files_copy[:num]  # assign the first num elements to the current fold\n",
    "        del files_copy[:num]  # now remove them from the list\n",
    "        \n",
    "    folds = [fold_0, fold_1, fold_2, fold_3, fold_4]\n",
    "    \n",
    "    if len(files_copy) == 0:\n",
    "        print('All scans assinged to their folds')\n",
    "    \n",
    "    return folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d84e8227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All scans assinged to their folds\n"
     ]
    }
   ],
   "source": [
    "folds = create_folds(files=files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0c76dd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[27, 27, 27, 28, 28]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(i) for i in folds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74ba2133",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/olli/Projects/Kaggle/Vesuvius/Data/Preprocessed/Cropped_Regions/1_5.pickle'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folds[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23432d7f",
   "metadata": {},
   "source": [
    "# 2) From these 5 folds create the 5 train and valid folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b81b3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_valid(folds):\n",
    "    \n",
    "    train_folds = [f'train_{i}' for i in range(5)]\n",
    "    valid_folds = [f'valid_{i}' for i in range(5)]\n",
    "    \n",
    "    # each time one unique fold is the validation-data and the rest is for training\n",
    "    for i in range(5):\n",
    "        folds_copy = deepcopy(folds)\n",
    "        \n",
    "        globals()[valid_folds[i]] = folds_copy.pop(i)  # current for for validation\n",
    "        \n",
    "        train = []  # append the 4 remaining 4 folds to the current train data\n",
    "        for fold in folds_copy:\n",
    "            train += fold\n",
    "            \n",
    "        # finally assing it to the variable\n",
    "        globals()[train_folds[i]] = train\n",
    "        \n",
    "    train_data = [train_0, train_1, train_2, train_3, train_4]\n",
    "    valid_data = [valid_0, valid_1, valid_2, valid_3, valid_4]\n",
    "    \n",
    "    return train_data, valid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01e9580d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data = create_train_valid(folds=folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "993b60c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[110, 110, 110, 109, 109]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(i) for i in train_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff2a0e3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[27, 27, 27, 28, 28]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(i) for i in valid_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09a8d3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure there is no validation data in the corresponding train data\n",
    "\n",
    "for i in range(5):\n",
    "    val = valid_data[i]\n",
    "    train = train_data[i]\n",
    "    \n",
    "    for file in val:\n",
    "        if file in train:\n",
    "            print(f'DUBLICATE: {file}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083f550c",
   "metadata": {},
   "source": [
    "# 3) Now create a model a simple 2D CNN for this segmentation task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dbebcf3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "04b51537",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smp.Unet(\n",
    "    encoder_name='se_resnext50_32x4d',        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    encoder_weights=None,     # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=5,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=1,                      # model output channels (number of classes in your dataset)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb6e8940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "====================================================================================================\n",
       "Layer (type:depth-idx)                             Output Shape              Param #\n",
       "====================================================================================================\n",
       "Unet                                               [1, 1, 1024, 1024]        --\n",
       "├─SENetEncoder: 1-1                                [1, 5, 1024, 1024]        --\n",
       "│    └─Sequential: 2-1                             --                        --\n",
       "│    │    └─Conv2d: 3-1                            [1, 64, 512, 512]         15,680\n",
       "│    │    └─BatchNorm2d: 3-2                       [1, 64, 512, 512]         128\n",
       "│    │    └─ReLU: 3-3                              [1, 64, 512, 512]         --\n",
       "│    │    └─MaxPool2d: 3-4                         [1, 64, 256, 256]         --\n",
       "│    └─Sequential: 2-2                             [1, 256, 256, 256]        --\n",
       "│    │    └─SEResNeXtBottleneck: 3-5               [1, 256, 256, 256]        71,952\n",
       "│    │    └─SEResNeXtBottleneck: 3-6               [1, 256, 256, 256]        79,632\n",
       "│    │    └─SEResNeXtBottleneck: 3-7               [1, 256, 256, 256]        79,632\n",
       "│    └─Sequential: 2-3                             [1, 512, 128, 128]        --\n",
       "│    │    └─SEResNeXtBottleneck: 3-8               [1, 512, 128, 128]        382,496\n",
       "│    │    └─SEResNeXtBottleneck: 3-9               [1, 512, 128, 128]        315,936\n",
       "│    │    └─SEResNeXtBottleneck: 3-10              [1, 512, 128, 128]        315,936\n",
       "│    │    └─SEResNeXtBottleneck: 3-11              [1, 512, 128, 128]        315,936\n",
       "│    └─Sequential: 2-4                             [1, 1024, 64, 64]         --\n",
       "│    │    └─SEResNeXtBottleneck: 3-12              [1, 1024, 64, 64]         1,522,752\n",
       "│    │    └─SEResNeXtBottleneck: 3-13              [1, 1024, 64, 64]         1,258,560\n",
       "│    │    └─SEResNeXtBottleneck: 3-14              [1, 1024, 64, 64]         1,258,560\n",
       "│    │    └─SEResNeXtBottleneck: 3-15              [1, 1024, 64, 64]         1,258,560\n",
       "│    │    └─SEResNeXtBottleneck: 3-16              [1, 1024, 64, 64]         1,258,560\n",
       "│    │    └─SEResNeXtBottleneck: 3-17              [1, 1024, 64, 64]         1,258,560\n",
       "│    └─Sequential: 2-5                             [1, 2048, 32, 32]         --\n",
       "│    │    └─SEResNeXtBottleneck: 3-18              [1, 2048, 32, 32]         6,076,544\n",
       "│    │    └─SEResNeXtBottleneck: 3-19              [1, 2048, 32, 32]         5,023,872\n",
       "│    │    └─SEResNeXtBottleneck: 3-20              [1, 2048, 32, 32]         5,023,872\n",
       "├─UnetDecoder: 1-2                                 [1, 16, 1024, 1024]       --\n",
       "│    └─Identity: 2-6                               [1, 2048, 32, 32]         --\n",
       "│    └─ModuleList: 2-7                             --                        --\n",
       "│    │    └─DecoderBlock: 3-21                     [1, 256, 64, 64]          7,668,736\n",
       "│    │    └─DecoderBlock: 3-22                     [1, 128, 128, 128]        1,032,704\n",
       "│    │    └─DecoderBlock: 3-23                     [1, 64, 256, 256]         258,304\n",
       "│    │    └─DecoderBlock: 3-24                     [1, 32, 512, 512]         46,208\n",
       "│    │    └─DecoderBlock: 3-25                     [1, 16, 1024, 1024]       6,976\n",
       "├─SegmentationHead: 1-3                            [1, 1, 1024, 1024]        --\n",
       "│    └─Conv2d: 2-8                                 [1, 1, 1024, 1024]        145\n",
       "│    └─Identity: 2-9                               [1, 1, 1024, 1024]        --\n",
       "│    └─Activation: 2-10                            [1, 1, 1024, 1024]        --\n",
       "│    │    └─Identity: 3-26                         [1, 1, 1024, 1024]        --\n",
       "====================================================================================================\n",
       "Total params: 34,530,241\n",
       "Trainable params: 34,530,241\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 174.72\n",
       "====================================================================================================\n",
       "Input size (MB): 20.97\n",
       "Forward/backward pass size (MB): 5863.77\n",
       "Params size (MB): 138.12\n",
       "Estimated Total Size (MB): 6022.86\n",
       "===================================================================================================="
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model, input_data=torch.randn(1, 5, 1024 , 1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "489e9ddf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UnetDecoder(\n",
       "  (center): Identity()\n",
       "  (blocks): ModuleList(\n",
       "    (0): DecoderBlock(\n",
       "      (conv1): Conv2dReLU(\n",
       "        (0): Conv2d(3072, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (attention1): Attention(\n",
       "        (attention): Identity()\n",
       "      )\n",
       "      (conv2): Conv2dReLU(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (attention2): Attention(\n",
       "        (attention): Identity()\n",
       "      )\n",
       "    )\n",
       "    (1): DecoderBlock(\n",
       "      (conv1): Conv2dReLU(\n",
       "        (0): Conv2d(768, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (attention1): Attention(\n",
       "        (attention): Identity()\n",
       "      )\n",
       "      (conv2): Conv2dReLU(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (attention2): Attention(\n",
       "        (attention): Identity()\n",
       "      )\n",
       "    )\n",
       "    (2): DecoderBlock(\n",
       "      (conv1): Conv2dReLU(\n",
       "        (0): Conv2d(384, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (attention1): Attention(\n",
       "        (attention): Identity()\n",
       "      )\n",
       "      (conv2): Conv2dReLU(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (attention2): Attention(\n",
       "        (attention): Identity()\n",
       "      )\n",
       "    )\n",
       "    (3): DecoderBlock(\n",
       "      (conv1): Conv2dReLU(\n",
       "        (0): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (attention1): Attention(\n",
       "        (attention): Identity()\n",
       "      )\n",
       "      (conv2): Conv2dReLU(\n",
       "        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (attention2): Attention(\n",
       "        (attention): Identity()\n",
       "      )\n",
       "    )\n",
       "    (4): DecoderBlock(\n",
       "      (conv1): Conv2dReLU(\n",
       "        (0): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (attention1): Attention(\n",
       "        (attention): Identity()\n",
       "      )\n",
       "      (conv2): Conv2dReLU(\n",
       "        (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (attention2): Attention(\n",
       "        (attention): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858b3f79",
   "metadata": {},
   "source": [
    "### The model has no sigmoid activation as output so use BCELossWithLogits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c61b9486",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 1024, 1024])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.randn(1, 5, 1024, 1024).to(device)).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cb711b",
   "metadata": {},
   "source": [
    "### Now save the initial weights to start a new model by loading these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c42994e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_weights(model):\n",
    "    name = 'UNET_random_weights.pth'\n",
    "    \n",
    "    path_weight = os.path.join(folder, 'Weights', name)\n",
    "    \n",
    "    if not os.path.exists(path_weight):\n",
    "        \n",
    "        torch.save(model.state_dict(), path_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "835ababd",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_weights(model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea2085f",
   "metadata": {},
   "source": [
    "# 4) Now define the functions to train and evaluate a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "31040bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, loss, X, y):\n",
    "    model.train()\n",
    "    \n",
    "    pred = model(X)\n",
    "    \n",
    "    batch_loss = loss(pred, y)\n",
    "    \n",
    "    batch_loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    return batch_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7494d3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def valid_loss(model, optimizer, loss, X, y):\n",
    "    model.eval()\n",
    "    \n",
    "    pred = model(X)\n",
    "    \n",
    "    batch_loss = loss(pred, y)\n",
    "    \n",
    "    return batch_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "65c0d630",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_dice(pred, y, beta=0.5, smooth=1e-5):\n",
    "\n",
    "    pred = torch.sigmoid(pred)  # model has no sigmoid\n",
    "\n",
    "    # create a single dimension float vector\n",
    "    pred = pred.view(-1).float()\n",
    "    y = y.view(-1).float()\n",
    "\n",
    "    y_true_count = y.sum()\n",
    "    ctp = pred[y==1].sum()\n",
    "    cfp = pred[y==0].sum()\n",
    "    beta_squared = beta * beta\n",
    "\n",
    "    c_precision = ctp / (ctp + cfp + smooth)\n",
    "    c_recall = ctp / (y_true_count + smooth)\n",
    "    dice = (1 + beta_squared) * (c_precision * c_recall) / (beta_squared * c_precision + c_recall + smooth)\n",
    "\n",
    "    return dice_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dd588290",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def eval_dice(model, X, y):\n",
    "    model.eval()\n",
    "    \n",
    "    pred = model(X)\n",
    "    \n",
    "    dice_score = calculate_dice(pred, y)\n",
    "    \n",
    "    return dice_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f42f9b",
   "metadata": {},
   "source": [
    "# 5) Define the augmentations (use albumentations with mask!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "91af7da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# diceloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ff16afdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentations_train = A.Compose([\n",
    "    A.RandomResizedCrop(height=1024, width=1024, scale=(0.75, 1)),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.Rotate(limit=20, p=0.8),\n",
    "    A.GaussNoise(p=0.8),\n",
    "    A.ElasticTransform(p=0.8),\n",
    "    A.Normalize(mean=[0.5] * 5, std=[0.5] * 5)  # value for each layer\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893f65d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f081f7e",
   "metadata": {},
   "source": [
    "# 6) Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "caa6dfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gets initialized with the paths from the corresponding five train/valid datasets\n",
    "\n",
    "class Data(Dataset):\n",
    "    def __init__(self, paths, transform=False):\n",
    "    \n",
    "        self.paths = paths\n",
    "        random.shuffle(self.paths)\n",
    "        \n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        path = self.paths[index]\n",
    "        \n",
    "        with open(path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "            \n",
    "        X, y = data\n",
    "        \n",
    "        y = y / 255.\n",
    "        y = torch.tensor(y).float()\n",
    "        \n",
    "        X = X[:5]  # 5 layers as defined\n",
    "        \n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410070e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3175afdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51743fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a21e77a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d795d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
