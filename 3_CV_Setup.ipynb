{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1928a4d3",
   "metadata": {},
   "source": [
    "# Notebook to create the Cross-Validation-Setup & train a first model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a60e63e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import cv2\n",
    "from glob import glob\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchinfo import summary\n",
    "from copy import deepcopy\n",
    "import segmentation_models_pytorch as smp\n",
    "from torchinfo import summary\n",
    "import random\n",
    "from copy import deepcopy\n",
    "import albumentations as A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b040211",
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = 5\n",
    "size = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5d7a286",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = '/home/olli/Projects/Kaggle/Vesuvius'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ec0be97",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_data = os.path.join(folder, 'Data', 'Preprocessed', 'Cropped_Regions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5dff864",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob(folder_data + '/*.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65b5b158",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "137"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d6fc227",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort them in the correct order to later get the same results\n",
    "files.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328e0ff8",
   "metadata": {},
   "source": [
    "# 1) Create 5 folds from the 137 cropped parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a738d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folds(files):\n",
    "    \n",
    "    files_copy = deepcopy(files)\n",
    "    \n",
    "    # shuffle them with the same random seed to get identical results\n",
    "    random.Random(42).shuffle(files_copy)\n",
    "    \n",
    "    # each fold will consist of 27, 27, 27, 28, 28 files\n",
    "    num_per_fold = [27, 27, 27, 28, 28]\n",
    "    folds = [f'fold_{i}' for i in range(5)]\n",
    "    \n",
    "    for num, fold in zip(num_per_fold, folds):\n",
    "        globals()[fold] = files_copy[:num]  # assign the first num elements to the current fold\n",
    "        del files_copy[:num]  # now remove them from the list\n",
    "        \n",
    "    folds = [fold_0, fold_1, fold_2, fold_3, fold_4]\n",
    "    \n",
    "    if len(files_copy) == 0:\n",
    "        print('All scans assinged to their folds')\n",
    "    \n",
    "    return folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d84e8227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All scans assinged to their folds\n"
     ]
    }
   ],
   "source": [
    "folds = create_folds(files=files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0c76dd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[27, 27, 27, 28, 28]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(i) for i in folds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74ba2133",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/olli/Projects/Kaggle/Vesuvius/Data/Preprocessed/Cropped_Regions/1_5.pickle'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folds[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23432d7f",
   "metadata": {},
   "source": [
    "# 2) From these 5 folds create the 5 train and valid folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b81b3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_valid(folds):\n",
    "    \n",
    "    train_folds = [f'train_{i}' for i in range(5)]\n",
    "    valid_folds = [f'valid_{i}' for i in range(5)]\n",
    "    \n",
    "    # each time one unique fold is the validation-data and the rest is for training\n",
    "    for i in range(5):\n",
    "        folds_copy = deepcopy(folds)\n",
    "        \n",
    "        globals()[valid_folds[i]] = folds_copy.pop(i)  # current for for validation\n",
    "        \n",
    "        train = []  # append the 4 remaining 4 folds to the current train data\n",
    "        for fold in folds_copy:\n",
    "            train += fold\n",
    "            \n",
    "        # finally assing it to the variable\n",
    "        globals()[train_folds[i]] = train\n",
    "        \n",
    "    train_data = [train_0, train_1, train_2, train_3, train_4]\n",
    "    valid_data = [valid_0, valid_1, valid_2, valid_3, valid_4]\n",
    "    \n",
    "    return train_data, valid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01e9580d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data = create_train_valid(folds=folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "993b60c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[110, 110, 110, 109, 109]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(i) for i in train_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff2a0e3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[27, 27, 27, 28, 28]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(i) for i in valid_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09a8d3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure there is no validation data in the corresponding train data\n",
    "\n",
    "for i in range(5):\n",
    "    val = valid_data[i]\n",
    "    train = train_data[i]\n",
    "    \n",
    "    for file in val:\n",
    "        if file in train:\n",
    "            print(f'DUBLICATE: {file}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083f550c",
   "metadata": {},
   "source": [
    "# 3) Now create a model a simple 2D CNN for this segmentation task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dbebcf3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "04b51537",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smp.Unet(\n",
    "    encoder_name='se_resnext50_32x4d',  # backbone encoder\n",
    "    encoder_weights=None,  # train from scratch\n",
    "    in_channels=5,  # input channels equals the depth, i.e. layer\n",
    "    classes=1,  # only 1 class (ink or no ink)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb6e8940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "====================================================================================================\n",
       "Layer (type:depth-idx)                             Output Shape              Param #\n",
       "====================================================================================================\n",
       "Unet                                               [1, 1, 224, 224]          --\n",
       "├─SENetEncoder: 1-1                                [1, 5, 224, 224]          --\n",
       "│    └─Sequential: 2-1                             --                        --\n",
       "│    │    └─Conv2d: 3-1                            [1, 64, 112, 112]         15,680\n",
       "│    │    └─BatchNorm2d: 3-2                       [1, 64, 112, 112]         128\n",
       "│    │    └─ReLU: 3-3                              [1, 64, 112, 112]         --\n",
       "│    │    └─MaxPool2d: 3-4                         [1, 64, 56, 56]           --\n",
       "│    └─Sequential: 2-2                             [1, 256, 56, 56]          --\n",
       "│    │    └─SEResNeXtBottleneck: 3-5               [1, 256, 56, 56]          71,952\n",
       "│    │    └─SEResNeXtBottleneck: 3-6               [1, 256, 56, 56]          79,632\n",
       "│    │    └─SEResNeXtBottleneck: 3-7               [1, 256, 56, 56]          79,632\n",
       "│    └─Sequential: 2-3                             [1, 512, 28, 28]          --\n",
       "│    │    └─SEResNeXtBottleneck: 3-8               [1, 512, 28, 28]          382,496\n",
       "│    │    └─SEResNeXtBottleneck: 3-9               [1, 512, 28, 28]          315,936\n",
       "│    │    └─SEResNeXtBottleneck: 3-10              [1, 512, 28, 28]          315,936\n",
       "│    │    └─SEResNeXtBottleneck: 3-11              [1, 512, 28, 28]          315,936\n",
       "│    └─Sequential: 2-4                             [1, 1024, 14, 14]         --\n",
       "│    │    └─SEResNeXtBottleneck: 3-12              [1, 1024, 14, 14]         1,522,752\n",
       "│    │    └─SEResNeXtBottleneck: 3-13              [1, 1024, 14, 14]         1,258,560\n",
       "│    │    └─SEResNeXtBottleneck: 3-14              [1, 1024, 14, 14]         1,258,560\n",
       "│    │    └─SEResNeXtBottleneck: 3-15              [1, 1024, 14, 14]         1,258,560\n",
       "│    │    └─SEResNeXtBottleneck: 3-16              [1, 1024, 14, 14]         1,258,560\n",
       "│    │    └─SEResNeXtBottleneck: 3-17              [1, 1024, 14, 14]         1,258,560\n",
       "│    └─Sequential: 2-5                             [1, 2048, 7, 7]           --\n",
       "│    │    └─SEResNeXtBottleneck: 3-18              [1, 2048, 7, 7]           6,076,544\n",
       "│    │    └─SEResNeXtBottleneck: 3-19              [1, 2048, 7, 7]           5,023,872\n",
       "│    │    └─SEResNeXtBottleneck: 3-20              [1, 2048, 7, 7]           5,023,872\n",
       "├─UnetDecoder: 1-2                                 [1, 16, 224, 224]         --\n",
       "│    └─Identity: 2-6                               [1, 2048, 7, 7]           --\n",
       "│    └─ModuleList: 2-7                             --                        --\n",
       "│    │    └─DecoderBlock: 3-21                     [1, 256, 14, 14]          7,668,736\n",
       "│    │    └─DecoderBlock: 3-22                     [1, 128, 28, 28]          1,032,704\n",
       "│    │    └─DecoderBlock: 3-23                     [1, 64, 56, 56]           258,304\n",
       "│    │    └─DecoderBlock: 3-24                     [1, 32, 112, 112]         46,208\n",
       "│    │    └─DecoderBlock: 3-25                     [1, 16, 224, 224]         6,976\n",
       "├─SegmentationHead: 1-3                            [1, 1, 224, 224]          --\n",
       "│    └─Conv2d: 2-8                                 [1, 1, 224, 224]          145\n",
       "│    └─Identity: 2-9                               [1, 1, 224, 224]          --\n",
       "│    └─Activation: 2-10                            [1, 1, 224, 224]          --\n",
       "│    │    └─Identity: 3-26                         [1, 1, 224, 224]          --\n",
       "====================================================================================================\n",
       "Total params: 34,530,241\n",
       "Trainable params: 34,530,241\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 8.36\n",
       "====================================================================================================\n",
       "Input size (MB): 1.00\n",
       "Forward/backward pass size (MB): 280.71\n",
       "Params size (MB): 138.12\n",
       "Estimated Total Size (MB): 419.84\n",
       "===================================================================================================="
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model, input_data=torch.randn(1, depth, size, size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "489e9ddf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UnetDecoder(\n",
       "  (center): Identity()\n",
       "  (blocks): ModuleList(\n",
       "    (0): DecoderBlock(\n",
       "      (conv1): Conv2dReLU(\n",
       "        (0): Conv2d(3072, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (attention1): Attention(\n",
       "        (attention): Identity()\n",
       "      )\n",
       "      (conv2): Conv2dReLU(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (attention2): Attention(\n",
       "        (attention): Identity()\n",
       "      )\n",
       "    )\n",
       "    (1): DecoderBlock(\n",
       "      (conv1): Conv2dReLU(\n",
       "        (0): Conv2d(768, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (attention1): Attention(\n",
       "        (attention): Identity()\n",
       "      )\n",
       "      (conv2): Conv2dReLU(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (attention2): Attention(\n",
       "        (attention): Identity()\n",
       "      )\n",
       "    )\n",
       "    (2): DecoderBlock(\n",
       "      (conv1): Conv2dReLU(\n",
       "        (0): Conv2d(384, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (attention1): Attention(\n",
       "        (attention): Identity()\n",
       "      )\n",
       "      (conv2): Conv2dReLU(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (attention2): Attention(\n",
       "        (attention): Identity()\n",
       "      )\n",
       "    )\n",
       "    (3): DecoderBlock(\n",
       "      (conv1): Conv2dReLU(\n",
       "        (0): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (attention1): Attention(\n",
       "        (attention): Identity()\n",
       "      )\n",
       "      (conv2): Conv2dReLU(\n",
       "        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (attention2): Attention(\n",
       "        (attention): Identity()\n",
       "      )\n",
       "    )\n",
       "    (4): DecoderBlock(\n",
       "      (conv1): Conv2dReLU(\n",
       "        (0): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (attention1): Attention(\n",
       "        (attention): Identity()\n",
       "      )\n",
       "      (conv2): Conv2dReLU(\n",
       "        (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (attention2): Attention(\n",
       "        (attention): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858b3f79",
   "metadata": {},
   "source": [
    "### The model has no sigmoid activation as output so use BCELossWithLogits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c61b9486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(model(torch.randn(1, depth, size, size).to(device)).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cb711b",
   "metadata": {},
   "source": [
    "### Now save the initial weights to start a new model by loading these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c42994e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_weights(model):\n",
    "    name = 'UNET_random_weights.pth'\n",
    "    \n",
    "    path_weight = os.path.join(folder, 'Weights', name)\n",
    "    \n",
    "    if not os.path.exists(path_weight):\n",
    "        \n",
    "        torch.save(model.state_dict(), path_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "835ababd",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_weights(model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea2085f",
   "metadata": {},
   "source": [
    "# 4) Now define the functions to train and evaluate a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "31040bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, loss, X, y):\n",
    "    model.train()\n",
    "    \n",
    "    pred = model(X)\n",
    "    \n",
    "    y = y.unsqueeze(1)\n",
    "    \n",
    "    batch_loss = loss(pred, y)\n",
    "    \n",
    "    batch_loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    return batch_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7494d3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def valid_loss(model, optimizer, loss, X, y):\n",
    "    model.eval()\n",
    "    \n",
    "    pred = model(X)\n",
    "    \n",
    "    y = y.unsqueeze(1)\n",
    "    \n",
    "    batch_loss = loss(pred, y)\n",
    "    \n",
    "    return batch_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "65c0d630",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_dice(pred, y, beta=0.5, smooth=1e-5):\n",
    "\n",
    "    pred = torch.sigmoid(pred)  # model has no sigmoid (bcelosswithlogits)\n",
    "\n",
    "    # create a single dimension float vector\n",
    "    pred = pred.view(-1).float()\n",
    "    y = y.view(-1).float()\n",
    "\n",
    "    y_true_count = y.sum()\n",
    "    ctp = pred[y==1].sum()\n",
    "    cfp = pred[y==0].sum()\n",
    "    beta_squared = beta * beta\n",
    "\n",
    "    c_precision = ctp / (ctp + cfp + smooth)\n",
    "    c_recall = ctp / (y_true_count + smooth)\n",
    "    dice_score = (1 + beta_squared) * (c_precision * c_recall) / (beta_squared * c_precision + c_recall + smooth)\n",
    "\n",
    "    return dice_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dd588290",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def eval_dice(model, X, y):\n",
    "    model.eval()\n",
    "    \n",
    "    pred = model(X)\n",
    "    \n",
    "    y = y.unsqueeze(1)\n",
    "    \n",
    "    dice_score = calculate_dice(pred, y)\n",
    "    \n",
    "    return dice_score.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f42f9b",
   "metadata": {},
   "source": [
    "# 5) Define the augmentations (use albumentations with mask!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "91af7da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  diceloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ff16afdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start only with minor augmentations\n",
    "augmentations_train = A.Compose([\n",
    "    A.Resize(size, size),\n",
    "    #A.RandomResizedCrop(height=1024, width=1024, scale=(0.75, 1)),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    #A.Rotate(limit=20, p=0.8, border_mode=cv2.BORDER_CONSTANT),\n",
    "    #A.GaussNoise(p=0.8),\n",
    "    #A.ElasticTransform(p=0.8),\n",
    "    A.Normalize(mean=[0.5] * depth, std=[0.5] * depth)  # value for each layer\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "893f65d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use augmentations on validation data only as normalizastion\n",
    "augmentations_valid = A.Compose([\n",
    "    A.Resize(size, size),\n",
    "    A.Normalize(mean=[0.5] * depth, std=[0.5] * depth)  # value for each layer\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5590da95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[0] * 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f081f7e",
   "metadata": {},
   "source": [
    "# 6) Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "caa6dfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gets initialized with the paths from the corresponding five train/valid datasets\n",
    "\n",
    "class Data(Dataset):\n",
    "    def __init__(self, paths, transform=False):\n",
    "    \n",
    "        self.paths = paths\n",
    "        random.shuffle(self.paths)\n",
    "        \n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        path = self.paths[index]\n",
    "        \n",
    "        with open(path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "            \n",
    "        X, y = data\n",
    "        \n",
    "        X = X[:, :, :depth]  # 5 layers as defined\n",
    "        \n",
    "        # transform the image as well as the label\n",
    "        if self.transform:\n",
    "            augmentation = self.transform(image=X, mask=y)\n",
    "            \n",
    "            X = augmentation['image']\n",
    "            y = augmentation['mask']\n",
    "\n",
    "        X = torch.tensor(X)\n",
    "        \n",
    "        X = X.permute(2, 0, 1).type(torch.float32)\n",
    "        \n",
    "        y = y / 255.\n",
    "        y = torch.tensor(y).float()\n",
    "        \n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f85aab",
   "metadata": {},
   "source": [
    "# Train/Validate 5 models on their data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5b5c2534",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 - Epoch - 0: Train Loss: 1.08, Train Dice: 0.25; Valid Loss: 0.75, Valid Dice: 0.24\n",
      "Model 1 - Epoch - 1: Train Loss: 0.99, Train Dice: 0.25; Valid Loss: 0.73, Valid Dice: 0.24\n",
      "Model 1 - Epoch - 2: Train Loss: 0.92, Train Dice: 0.26; Valid Loss: 0.73, Valid Dice: 0.25\n",
      "Model 1 - Epoch - 3: Train Loss: 0.84, Train Dice: 0.27; Valid Loss: 0.71, Valid Dice: 0.25\n",
      "Model 1 - Epoch - 4: Train Loss: 0.79, Train Dice: 0.28; Valid Loss: 0.74, Valid Dice: 0.27\n",
      "Model 1 - Epoch - 5: Train Loss: 0.75, Train Dice: 0.29; Valid Loss: 0.72, Valid Dice: 0.27\n",
      "Model 1 - Epoch - 6: Train Loss: 0.70, Train Dice: 0.29; Valid Loss: 0.77, Valid Dice: 0.28\n",
      "Model 1 - Epoch - 7: Train Loss: 0.67, Train Dice: 0.28; Valid Loss: 0.73, Valid Dice: 0.27\n",
      "Model 1 - Epoch - 8: Train Loss: 0.65, Train Dice: 0.29; Valid Loss: 0.68, Valid Dice: 0.27\n",
      "Model 1 - Epoch - 9: Train Loss: 0.63, Train Dice: 0.28; Valid Loss: 0.67, Valid Dice: 0.27\n",
      "Model 1 - Epoch - 10: Train Loss: 0.60, Train Dice: 0.26; Valid Loss: 0.60, Valid Dice: 0.24\n",
      "Model 1 - Epoch - 11: Train Loss: 0.58, Train Dice: 0.29; Valid Loss: 0.60, Valid Dice: 0.25\n",
      "Model 1 - Epoch - 12: Train Loss: 0.57, Train Dice: 0.30; Valid Loss: 0.64, Valid Dice: 0.27\n",
      "Model 1 - Epoch - 13: Train Loss: 0.54, Train Dice: 0.30; Valid Loss: 0.58, Valid Dice: 0.25\n",
      "Model 1 - Epoch - 14: Train Loss: 0.53, Train Dice: 0.33; Valid Loss: 0.61, Valid Dice: 0.26\n",
      "Model 1 - Epoch - 15: Train Loss: 0.55, Train Dice: 0.34; Valid Loss: 0.65, Valid Dice: 0.26\n",
      "Model 1 - Epoch - 16: Train Loss: 0.50, Train Dice: 0.33; Valid Loss: 1.23, Valid Dice: 0.27\n",
      "Model 1 - Epoch - 17: Train Loss: 0.51, Train Dice: 0.29; Valid Loss: 0.89, Valid Dice: 0.24\n",
      "Model 1 - Epoch - 18: Train Loss: 0.52, Train Dice: 0.30; Valid Loss: 0.66, Valid Dice: 0.24\n",
      "Model 1 - Epoch - 19: Train Loss: 0.51, Train Dice: 0.36; Valid Loss: 0.63, Valid Dice: 0.26\n",
      "Model 2 - Epoch - 0: Train Loss: 1.11, Train Dice: 0.24; Valid Loss: 0.73, Valid Dice: 0.25\n",
      "Model 2 - Epoch - 1: Train Loss: 1.01, Train Dice: 0.24; Valid Loss: 0.69, Valid Dice: 0.25\n",
      "Model 2 - Epoch - 2: Train Loss: 0.92, Train Dice: 0.24; Valid Loss: 0.67, Valid Dice: 0.25\n",
      "Model 2 - Epoch - 3: Train Loss: 0.85, Train Dice: 0.25; Valid Loss: 0.66, Valid Dice: 0.26\n",
      "Model 2 - Epoch - 4: Train Loss: 0.80, Train Dice: 0.26; Valid Loss: 0.66, Valid Dice: 0.26\n",
      "Model 2 - Epoch - 5: Train Loss: 0.74, Train Dice: 0.25; Valid Loss: 0.64, Valid Dice: 0.26\n",
      "Model 2 - Epoch - 6: Train Loss: 0.72, Train Dice: 0.26; Valid Loss: 0.64, Valid Dice: 0.27\n",
      "Model 2 - Epoch - 7: Train Loss: 0.66, Train Dice: 0.28; Valid Loss: 0.65, Valid Dice: 0.28\n",
      "Model 2 - Epoch - 8: Train Loss: 0.64, Train Dice: 0.26; Valid Loss: 0.62, Valid Dice: 0.26\n",
      "Model 2 - Epoch - 9: Train Loss: 0.62, Train Dice: 0.26; Valid Loss: 0.61, Valid Dice: 0.26\n",
      "Model 2 - Epoch - 10: Train Loss: 0.60, Train Dice: 0.27; Valid Loss: 0.61, Valid Dice: 0.28\n",
      "Model 2 - Epoch - 11: Train Loss: 0.62, Train Dice: 0.30; Valid Loss: 0.69, Valid Dice: 0.30\n",
      "Model 2 - Epoch - 12: Train Loss: 0.57, Train Dice: 0.28; Valid Loss: 0.88, Valid Dice: 0.26\n",
      "Model 2 - Epoch - 13: Train Loss: 0.56, Train Dice: 0.31; Valid Loss: 1.04, Valid Dice: 0.30\n",
      "Model 2 - Epoch - 14: Train Loss: 0.55, Train Dice: 0.29; Valid Loss: 0.75, Valid Dice: 0.26\n",
      "Model 2 - Epoch - 15: Train Loss: 0.56, Train Dice: 0.34; Valid Loss: 0.68, Valid Dice: 0.30\n",
      "Model 2 - Epoch - 16: Train Loss: 0.53, Train Dice: 0.29; Valid Loss: 0.56, Valid Dice: 0.25\n",
      "Model 2 - Epoch - 17: Train Loss: 0.52, Train Dice: 0.34; Valid Loss: 0.59, Valid Dice: 0.29\n",
      "Model 2 - Epoch - 18: Train Loss: 0.50, Train Dice: 0.28; Valid Loss: 0.55, Valid Dice: 0.24\n",
      "Model 2 - Epoch - 19: Train Loss: 0.50, Train Dice: 0.34; Valid Loss: 0.57, Valid Dice: 0.28\n",
      "Model 3 - Epoch - 0: Train Loss: 1.09, Train Dice: 0.24; Valid Loss: 0.75, Valid Dice: 0.23\n",
      "Model 3 - Epoch - 1: Train Loss: 0.99, Train Dice: 0.25; Valid Loss: 0.72, Valid Dice: 0.23\n",
      "Model 3 - Epoch - 2: Train Loss: 0.92, Train Dice: 0.26; Valid Loss: 0.69, Valid Dice: 0.23\n",
      "Model 3 - Epoch - 3: Train Loss: 0.86, Train Dice: 0.26; Valid Loss: 0.68, Valid Dice: 0.24\n",
      "Model 3 - Epoch - 4: Train Loss: 0.79, Train Dice: 0.28; Valid Loss: 0.72, Valid Dice: 0.25\n",
      "Model 3 - Epoch - 5: Train Loss: 0.74, Train Dice: 0.29; Valid Loss: 0.75, Valid Dice: 0.26\n",
      "Model 3 - Epoch - 6: Train Loss: 0.69, Train Dice: 0.30; Valid Loss: 0.82, Valid Dice: 0.27\n",
      "Model 3 - Epoch - 7: Train Loss: 0.68, Train Dice: 0.28; Valid Loss: 0.68, Valid Dice: 0.26\n",
      "Model 3 - Epoch - 8: Train Loss: 0.66, Train Dice: 0.26; Valid Loss: 0.60, Valid Dice: 0.23\n",
      "Model 3 - Epoch - 9: Train Loss: 0.63, Train Dice: 0.26; Valid Loss: 0.59, Valid Dice: 0.24\n",
      "Model 3 - Epoch - 10: Train Loss: 0.61, Train Dice: 0.27; Valid Loss: 0.60, Valid Dice: 0.24\n",
      "Model 3 - Epoch - 11: Train Loss: 0.58, Train Dice: 0.29; Valid Loss: 0.62, Valid Dice: 0.25\n",
      "Model 3 - Epoch - 12: Train Loss: 0.58, Train Dice: 0.30; Valid Loss: 0.65, Valid Dice: 0.26\n",
      "Model 3 - Epoch - 13: Train Loss: 0.55, Train Dice: 0.32; Valid Loss: 0.82, Valid Dice: 0.26\n",
      "Model 3 - Epoch - 14: Train Loss: 0.54, Train Dice: 0.32; Valid Loss: 0.68, Valid Dice: 0.25\n",
      "Model 3 - Epoch - 15: Train Loss: 0.55, Train Dice: 0.31; Valid Loss: 0.63, Valid Dice: 0.24\n",
      "Model 3 - Epoch - 16: Train Loss: 0.53, Train Dice: 0.31; Valid Loss: 0.61, Valid Dice: 0.23\n",
      "Model 3 - Epoch - 17: Train Loss: 0.51, Train Dice: 0.34; Valid Loss: 0.60, Valid Dice: 0.25\n",
      "Model 3 - Epoch - 18: Train Loss: 0.51, Train Dice: 0.29; Valid Loss: 0.53, Valid Dice: 0.22\n",
      "Model 3 - Epoch - 19: Train Loss: 0.50, Train Dice: 0.36; Valid Loss: 0.56, Valid Dice: 0.24\n",
      "Model 4 - Epoch - 0: Train Loss: 1.10, Train Dice: 0.24; Valid Loss: 0.74, Valid Dice: 0.26\n",
      "Model 4 - Epoch - 1: Train Loss: 1.00, Train Dice: 0.24; Valid Loss: 0.71, Valid Dice: 0.26\n",
      "Model 4 - Epoch - 2: Train Loss: 0.93, Train Dice: 0.25; Valid Loss: 0.70, Valid Dice: 0.26\n",
      "Model 4 - Epoch - 3: Train Loss: 0.86, Train Dice: 0.27; Valid Loss: 0.70, Valid Dice: 0.27\n",
      "Model 4 - Epoch - 4: Train Loss: 0.79, Train Dice: 0.27; Valid Loss: 0.71, Valid Dice: 0.28\n",
      "Model 4 - Epoch - 5: Train Loss: 0.75, Train Dice: 0.28; Valid Loss: 0.75, Valid Dice: 0.29\n",
      "Model 4 - Epoch - 6: Train Loss: 0.71, Train Dice: 0.29; Valid Loss: 0.80, Valid Dice: 0.30\n",
      "Model 4 - Epoch - 7: Train Loss: 0.67, Train Dice: 0.29; Valid Loss: 0.73, Valid Dice: 0.29\n",
      "Model 4 - Epoch - 8: Train Loss: 0.65, Train Dice: 0.27; Valid Loss: 0.66, Valid Dice: 0.29\n",
      "Model 4 - Epoch - 9: Train Loss: 0.62, Train Dice: 0.29; Valid Loss: 0.66, Valid Dice: 0.29\n",
      "Model 4 - Epoch - 10: Train Loss: 0.62, Train Dice: 0.28; Valid Loss: 0.66, Valid Dice: 0.29\n",
      "Model 4 - Epoch - 11: Train Loss: 0.58, Train Dice: 0.29; Valid Loss: 0.66, Valid Dice: 0.29\n",
      "Model 4 - Epoch - 12: Train Loss: 0.57, Train Dice: 0.28; Valid Loss: 0.62, Valid Dice: 0.28\n",
      "Model 4 - Epoch - 13: Train Loss: 0.58, Train Dice: 0.30; Valid Loss: 0.60, Valid Dice: 0.27\n",
      "Model 4 - Epoch - 14: Train Loss: 0.55, Train Dice: 0.30; Valid Loss: 0.98, Valid Dice: 0.28\n",
      "Model 4 - Epoch - 15: Train Loss: 0.55, Train Dice: 0.28; Valid Loss: 0.70, Valid Dice: 0.25\n",
      "Model 4 - Epoch - 16: Train Loss: 0.54, Train Dice: 0.29; Valid Loss: 0.58, Valid Dice: 0.26\n",
      "Model 4 - Epoch - 17: Train Loss: 0.53, Train Dice: 0.32; Valid Loss: 0.59, Valid Dice: 0.28\n",
      "Model 4 - Epoch - 18: Train Loss: 0.51, Train Dice: 0.29; Valid Loss: 0.56, Valid Dice: 0.25\n",
      "Model 4 - Epoch - 19: Train Loss: 0.51, Train Dice: 0.31; Valid Loss: 0.55, Valid Dice: 0.25\n",
      "Model 5 - Epoch - 0: Train Loss: 1.10, Train Dice: 0.24; Valid Loss: 0.74, Valid Dice: 0.25\n",
      "Model 5 - Epoch - 1: Train Loss: 1.02, Train Dice: 0.25; Valid Loss: 0.72, Valid Dice: 0.26\n",
      "Model 5 - Epoch - 2: Train Loss: 0.95, Train Dice: 0.25; Valid Loss: 0.71, Valid Dice: 0.26\n",
      "Model 5 - Epoch - 3: Train Loss: 0.88, Train Dice: 0.25; Valid Loss: 0.69, Valid Dice: 0.27\n",
      "Model 5 - Epoch - 4: Train Loss: 0.82, Train Dice: 0.26; Valid Loss: 0.68, Valid Dice: 0.28\n",
      "Model 5 - Epoch - 5: Train Loss: 0.78, Train Dice: 0.27; Valid Loss: 0.69, Valid Dice: 0.28\n",
      "Model 5 - Epoch - 6: Train Loss: 0.73, Train Dice: 0.28; Valid Loss: 0.74, Valid Dice: 0.30\n",
      "Model 5 - Epoch - 7: Train Loss: 0.69, Train Dice: 0.27; Valid Loss: 0.69, Valid Dice: 0.29\n",
      "Model 5 - Epoch - 8: Train Loss: 0.65, Train Dice: 0.25; Valid Loss: 0.63, Valid Dice: 0.27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 5 - Epoch - 9: Train Loss: 0.62, Train Dice: 0.25; Valid Loss: 0.62, Valid Dice: 0.27\n",
      "Model 5 - Epoch - 10: Train Loss: 0.61, Train Dice: 0.28; Valid Loss: 0.66, Valid Dice: 0.29\n",
      "Model 5 - Epoch - 11: Train Loss: 0.58, Train Dice: 0.28; Valid Loss: 0.66, Valid Dice: 0.28\n",
      "Model 5 - Epoch - 12: Train Loss: 0.56, Train Dice: 0.26; Valid Loss: 0.62, Valid Dice: 0.26\n",
      "Model 5 - Epoch - 13: Train Loss: 0.56, Train Dice: 0.27; Valid Loss: 0.60, Valid Dice: 0.27\n",
      "Model 5 - Epoch - 14: Train Loss: 0.54, Train Dice: 0.27; Valid Loss: 0.59, Valid Dice: 0.25\n",
      "Model 5 - Epoch - 15: Train Loss: 0.52, Train Dice: 0.29; Valid Loss: 0.68, Valid Dice: 0.26\n",
      "Model 5 - Epoch - 16: Train Loss: 0.51, Train Dice: 0.32; Valid Loss: 0.98, Valid Dice: 0.29\n",
      "Model 5 - Epoch - 17: Train Loss: 0.49, Train Dice: 0.31; Valid Loss: 0.75, Valid Dice: 0.27\n",
      "Model 5 - Epoch - 18: Train Loss: 0.50, Train Dice: 0.35; Valid Loss: 0.68, Valid Dice: 0.28\n",
      "Model 5 - Epoch - 19: Train Loss: 0.48, Train Dice: 0.37; Valid Loss: 0.63, Valid Dice: 0.28\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "learning_rate = 1e-4\n",
    "batch_size = 32\n",
    "\n",
    "df = pd.DataFrame()  # empty dataframe to store all the metrics\n",
    "\n",
    "for i in range(5):\n",
    "    \n",
    "    # for each model load the random weights to start again\n",
    "    path_random_weights = os.path.join(folder, 'Weights', 'UNET_random_weights.pth')\n",
    "    model.load_state_dict(torch.load(path_random_weights))\n",
    "    \n",
    "    # get the current datasets\n",
    "    train = train_data[i]\n",
    "    valid = valid_data[i]\n",
    "    \n",
    "    # create a train and valid dataset\n",
    "    train_ds = Data(paths=train, transform=augmentations_train)\n",
    "    valid_ds = Data(paths=valid, transform=augmentations_valid)\n",
    "    \n",
    "    # create the dataloaders for both datasets\n",
    "    train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=16)\n",
    "    valid_dl = DataLoader(valid_ds, batch_size=batch_size, num_workers=10)\n",
    "    \n",
    "    # define the loss\n",
    "    bce_loss = nn.BCEWithLogitsLoss()  # ToDo: Weights for both classes\n",
    "    \n",
    "    # define an optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # to later plot the metrics put them in a list after each epoch\n",
    "    epoch_train_loss, epoch_valid_loss = [], []\n",
    "    epoch_train_dice, epoch_valid_dice = [], []\n",
    "    \n",
    "    # train the model for num_epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        train_losses, valid_losses = [], []\n",
    "        train_dices, valid_dices = [], []\n",
    "        \n",
    "        # train the model with all the training data\n",
    "        for index, (X, y) in enumerate(train_dl):\n",
    "\n",
    "            # put the current batch to the gpu\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            batch_loss = train_model(model, optimizer, bce_loss, X, y)\n",
    "            train_losses.append(batch_loss)\n",
    "            \n",
    "        # get the mean of the losses and add it to the epoch losses\n",
    "        train_losses_mean = np.array(train_losses).mean()\n",
    "        epoch_train_loss.append(train_losses_mean)\n",
    "        \n",
    "        # calculate the dice score with all training samples\n",
    "        for index, (X, y) in enumerate(train_dl):\n",
    "\n",
    "            # put the current batch to the gpu\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            dice_score = eval_dice(model, X, y)\n",
    "            train_dices.append(dice_score)\n",
    "            \n",
    "        # get the mean of the losses and add it to the epoch losses\n",
    "        train_dice_mean = np.array(train_dices).mean()\n",
    "        epoch_train_dice.append(train_dice_mean)\n",
    "        \n",
    "        # do the same for the validation data\n",
    "        for index, (X, y) in enumerate(valid_dl):\n",
    "            \n",
    "            # put the current batch to the gpu\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            batch_loss = valid_loss(model, optimizer, bce_loss, X, y)\n",
    "            valid_losses.append(batch_loss)\n",
    "            \n",
    "        valid_loss_mean = np.array(valid_losses).mean()\n",
    "        epoch_valid_loss.append(valid_loss_mean)\n",
    "        \n",
    "        # do the same for the validation data\n",
    "        for index, (X, y) in enumerate(valid_dl):\n",
    "            \n",
    "            # put the current batch to the gpu\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            dice_score  = eval_dice(model, X, y)\n",
    "            valid_dices.append(dice_score)\n",
    "            \n",
    "        valid_dice_mean = np.array(valid_dices).mean()\n",
    "        epoch_valid_dice.append(valid_dice_mean)\n",
    "        \n",
    "        # print some progress after each epoch\n",
    "        print(f'Model {i + 1} - Epoch - {epoch}: ', end='')\n",
    "        print(f'Train Loss: {train_losses_mean:.2f}, Train Dice: {train_dice_mean:.2f}; ', end='')\n",
    "        print(f'Valid Loss: {valid_loss_mean:.2f}, Valid Dice: {valid_dice_mean:.2f}')\n",
    "    \n",
    "    # save the metrics after the model finished training\n",
    "    df[f'model_{i}_train_loss'] = epoch_train_loss\n",
    "    df[f'model_{i}_train_dice'] = epoch_train_dice\n",
    "    df[f'model_{i}_valid_loss'] = epoch_valid_loss\n",
    "    df[f'model_{i}_valid_dice'] = epoch_valid_dice\n",
    "    \n",
    "    # also save the weights\n",
    "    path_new_weights = os.path.join(folder, 'Weights', f'Model_{i + 1}.pth')\n",
    "    torch.save(model.state_dict(), path_new_weights)\n",
    "    \n",
    "    print('\\n\\n')\n",
    "    \n",
    "# save the df\n",
    "path_df = os.path.join(folder, 'Metrics', 'Notebook_3.csv')\n",
    "df.to_csv(path_df, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140d86ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
